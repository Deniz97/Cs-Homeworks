% Template LaTeX source file for homework problem solutions.
% Alan T. Sherman (9/9/98)

% Running LaTeX
%
% Name this file FOO.tex
% latex FOO
% latex FOO   
%    (You have to run latex twice to get the cross references correct.
%     Running latex creates a file FOO.dvi 
%     You can view dvi files with the program xdvi )
% xdvi FOO.dvi &
%
% lpr -d FOO.dvi
%    (To print the dvi file.   Be sure to use the "-d" print option,
%     and be sure your printer can handle dvi files (not all printers can).
%     Do NOT print with "lpr FOO.dvi", which will print tens of pages
%     of unreadable dvi source code. Printing a postscript (ps) file
%     is usually more reliable, as explained below.)
%
% dvips FOO.dvi
%    (To create a postscript file named FOO.ps 
%     which you can view with the program ghostview )
% ghostview FOO.ps &
% lpr FOO.ps
%    (To print the ps file.)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{article}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}

% Set the margins
%
\setlength{\textheight}{8.5in}
\setlength{\headheight}{.25in}
\setlength{\headsep}{.25in}
\setlength{\topmargin}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Macros

% Math Macros.  It would be better to use the AMS LaTeX package,
% including the Bbb fonts, but I'm showing how to get by with the most
% primitive version of LaTeX.  I follow the naming convention to begin
% user-defined macro and variable names with the prefix "my" to make it
% easier to distiguish user-defined macros from LaTeX commands.
%
\newcommand{\myN}{\hbox{N\hspace*{-.9em}I\hspace*{.4em}}}
\newcommand{\myZ}{\hbox{Z}^+}
\newcommand{\myR}{\hbox{R}}

\newcommand{\myfunction}[3]
{${#1} : {#2} \rightarrow {#3}$ }

\newcommand{\myzrfunction}[1]
{\myfunction{#1}{{\myZ}}{{\myR}}}


% Formating Macros
%

\newcommand{\myheader}[4]
{\vspace*{-0.5in}
\noindent
{#1} \hfill {#3}

\noindent
{#2} \hfill {#4}

\noindent
\rule[8pt]{\textwidth}{1pt}

\vspace{1ex} 
}  % end \myheader 

\newcommand{\myalgsheader}[0]
{\myheader{METU, Computer Engineering}
{CENG564 - Pattern Recognition THE "2" \\ "{\bf Partner 1 Name}"  - "Partner 1 ID" "{\bf Partner 2 Name}"  - "Partner 2 ID"} {Fall 2016}{}}

% Running head (goes at top of each page, beginning with page 2.
% Must precede by \pagestyle{myheadings}.
\newcommand{\myrunninghead}[2]
{\markright{{\it {#1}, {#2}}}}




%%%%%% Begin document with header and title %%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\myalgsheader
\pagenumbering{gobble}
\pagestyle{plain}



\section*{Questions} 

% fill the blanks with your answers
\section*{Answer 1}
% answer of question 1 goes here etc..

Let $\bar{X} = \{3,5,7\}$, $\bar{Z} = \{0,1\}$, $\bar{Y} = \{10,20,30\}$. Where the sets are the possible values of the respective random variables. We assume that all other probabilities of other values for each random variable is zero. This is correct since respective columns and rows add up to 1 at each case.\\

i. $P(Z=1 | X=7) =  \dfrac{(Z=1, X=7)}{P(X=7)} = \dfrac{0.21}{0.30}=0.7$ \\ Where $P(X=7) = \sum_{i\in \bar{Y},j\in \bar{Z}  } P(X=7,Y=i,Z=j)$ which we can infer each term directly from the table.\\

ii. 
\[ P(X=x) = \begin{cases} 
      0.3 & x = 3 \\
      0.4 & x=5 \\
      0.3 & x=7 \\
      0 & otherwise 
   \end{cases}
\] \\

    Where for all $n \in \bar{X}$, we can infer $P(X=n) = \sum_{i\in \bar{Y},j\in \bar{Z}  } P(X=n,Y=i,Z=j)$ where we know the each joint probability term by the given table.\\
    
        iii. 
    $E_x[X] = \sum_{i\in\bar{X}} P(i)i = 3 \cdot \dfrac{3}{10}+ 5\cdot \dfrac{4}{10}+ 7\cdot \dfrac{3}{10} = 5 $ \\
    
    Let us denote this expected value as $\mu_x$ \\
    
    $Var_x[X] = E_x[ (X - \mu_x)^2 ] = \sum_{i\in\bar{X}} P( i )(i-\mu_x)^2 = 0.3\cdot 4 + 0.4\cdot 0 + 0.3 \cdot 4 = 2.4$ \\
    
%    Where it can be shown that the term $P( (i-\mu^2) )$ is equal to just $P(i)$
    iv.
	For all $n\in \bar{X}$,	$P( X = n | Z=1) = \dfrac{P(X=n, Z=1)  }{P(Z=1)}$ \\
	
	For the denominator, $P(Z=1) = \sum_{i\in \bar{Y},j\in \bar{X}  } P(X=j,Y=i,Z=1)= 0.685$ , where we can infer each term directly from the table. \\
	
	For numerator, $P(X=n, Z=1) =  \sum_{i\in \bar{Y}} P(X=n,Y=i,Z=1)$ where again we can infer each term directly from the table. \\
	
	Putting all together and rounding up to 4 significant value; 
	\[ P(X =x | Z = 1 ) = \begin{cases} 
       0.2847 & x = 3 \\
      0.4088 & x=5 \\
      0.3066 & x=7 \\
      0 & otherwise 
   \end{cases}
\] \\

       v.
    $E_{X|Z=1}[x] = \sum_{i\in\bar{X}} P(i|Z=1)i = 3 \cdot 0.2847+ 5\cdot 0.4088+ 7\cdot 0.3066 = 5.0438 $ \\
    
    Let us denote this expected value as $\mu_{x2}$ \\
    
    $Var_x[X] = E_x[ (X - \mu_{x2})^2 ] = \sum_{i\in\bar{X}} P( i )(i-\mu_{x2})^2 = 0.2847\cdot 4.1771 + 0.4088\cdot 0.0019 + 0.3066 \cdot 3.8267 = 2.3630$ \\
    
\section*{Answer 2}

Assuming $X$ is choosen uniform randomly;

i. By the pdf of the uniform distribution:

	\[ f( x ) = \begin{cases} 
       \dfrac{1}{90} & x \in [10,100] \\
      0 & otherwise 
   \end{cases}
\] \\
% \displaystyle\int\limits_a^b  
And $P(I=[a,b]) = P(a\leq x \leq b) = \displaystyle\int\limits_a^b  f(x)dx = \displaystyle\int\limits_a^b   \dfrac{1}{90} = \dfrac{b-a}{90} $ \\

ii. $E_X[x] = \displaystyle\int\limits_{10}^{100} xf(x)dx =  \displaystyle\int\limits_{10}^{100}  \dfrac{x}{90}dx =\frac{500}{9}-\frac{5}{9} = 55   $ \\

iii. It is easy to show that the roots of the equation are $40$ and $70$, and the equation is positive between $[10,40)$ and $(70,100]$ and negative in $(40,70)$. Given that, the probabilty of the equation being positive is: \\

$P(X^2-110X+2800>0) = 1 - P(40 \leq x \leq 70) = 1 -  \dfrac{70-40}{90} = 0.6666 $ by part i. \\

Also note that since the probability of "single points" are zero for continous distributions, like this one, $\leq$ and $<$ makes no difference above. \\

iv. The integral of the density function should equal zero. Therefore: \\

$  \displaystyle\int\limits_{-\infty}^{+\infty} f(x)dx  = \displaystyle\int\limits_{10}^{100} f(x)dx = \displaystyle\int\limits_{10}^{100}Cxdx =C ( \frac{100^2}{2}-\frac{10^2}{2}) = C\cdot 4950 = 1$ \\ 

Therefore, $C=\dfrac{1}{4950} $ and $f(x) = \d1frac{x}{4950} $ \\

And $E_X[x] = \displaystyle\int\limits_{10}^{100} xf(x)dx =  \displaystyle\int\limits_{10}^{100}  \dfrac{x^2}{4950}dx =\frac{20000}{297}-\frac{20}{297} = \frac{740}{11} $ \\



\section*{Answer 3} 

\quad i. Let $Y=\sqrt{|X|} $ be a random variable (as it already is). We can start by considering cdf of $Y$. Let $F_X(x)$ be the cdf of $X$ and $F_Y(x)$ of Y. It is easy to see that $Y$ is always positive, and $Y$ is "bounded" by the probabilty of $X$ which means $f_Y(x)=0$ for $x\notin [-1,1]$. So assume $x\in [0,1]$: \\

$F_Y(x) = P(Y<x) = P( \sqrt{|X|} < x) = P( |X| < x^2 ) = P( -x^2 < X < x^2 ) = $ \\ 

$P( X < x^2) - P( X < -x^2 ) = F_X(x^2) - F_X(-x^2) $ \\ 


The third equality is possible because square root is a monotonically increasing function. Taking derivative of both sides: \\

$f_Y(x) =2xf_X(x^2)+2xf_X(-x^2)$ \\

Note that when $x\in [-1,1]$, so does $x^2$ and $-x^2$ too. Then since we know $f_X$ by the definition of pdf of uniform random variable:\\

$f_Y(x) = 2x(\dfrac{1}{2}+\dfrac{1}{2})=2x $

\[ f_Y( x ) = \begin{cases} 
		2x & 0 < x < 1        \\
       0 & otherwise
      
   \end{cases}
\] \\


ii. Let $Z= -ln|X|  $ be a random variable (as it still already is). We can still start by considering cdf of $Z$. Let $F_X(x)$ be the cdf of $X$ and $F_Z(x)$ of Z.\\

$F_Z(x) = P(Z<x) = P( -ln|X| < x) = P( ln|X| > -x) =P( |X| > e^{-x} ) =$\\

$ 1 - P( |X| < e^{-x} ) = 1 - P( e^{-x} < X < e^{-x} ) = 1- P( X < e^{-x} ) + P( X < -e^{-x} ) = $\\

$1-F_X(e^{-x}) + F_X(-e^{-x})$\\


The fourth equality is possible because $ln$ is a monotonically increasing function. Taking derivative of both sides: \\

$f_Z(x) =e^{-x}f_X(e^{-x})+ e^{-x} f_X(-e^{-x})$ \\

Notice that when $e^{-x}\in [-1,1]$, $x\in [0,\infty)$. When $-e^{-x}\in [-1,1]$, $x\in [0,\infty)$. So take $x$ in that region:\\

$f_Z(x) =e^{-x}  (\dfrac{1}{2}+\dfrac{1}{2}) =e^{-x} $

\[ f_Z( x ) = \begin{cases} 
		e^{-x} & 0 < x \\
       0 & otherwise
      
   \end{cases}
\] \\

Also note that, again since the random variables are contionus, $\leq$ and $<$ makes no difference.




\section*{Answer 4}

Assuming the coordinates are on the edges of the triangle, this implies $X \sim U(0,1)$ and $Y \sim U(-1,1)$. \\

Take $Z=X-Y$ (we will include absolute operator later). \\

$F_Z(z) = P(Z<z) = \displaystyle\int\limits_{-\infty}^{+\infty} P(Z<z|X=x)dF_X(x)  $\\

$ = \displaystyle\int\limits_{-\infty}^{+\infty} P(X-Y<z | X=x)dF_X(x) $ \\

Notice that since it is given $X=x$, $X=x$.\\

$ = \displaystyle\int\limits_{-\infty}^{+\infty} P(x-Y<z | X=x)dF_X(x) $ \\

$ = \displaystyle\int\limits_{-\infty}^{+\infty} P( x-z <Y )dF_X(x) $ \\

$ = \displaystyle\int\limits_{-\infty}^{+\infty} ( 1-  F_Y(x-z) )  dF_X(x) $ \\

Now, what we have here is a Riemann-Stieltjes integral. Since by definition $F_X$ is differentiable, we can write:

$ = \displaystyle\int\limits_{-\infty}^{+\infty} ( 1-  F_Y(x-z) ) f_X(x)dx $ \\ 

Now it is given that $X$ is a uniform distribution, and since it is on the given triangle its range is $[0,1]$. Let's break our integral in to three pieces, as $f_X(x) = 1$ when $x\in [0,1]$ and $0$ otherwise:

$ =\displaystyle\int\limits_{-\infty}^{0} ( 1- F_Y(x-z) ) f_X(x) dx + \displaystyle\int\limits_{0}^{1} ( 1- F_Y(x-z) ) f_X(x) dx $\\$+ \displaystyle\int\limits_{1}^{+\infty} ( 1- F_Y(x-z) ) f_X(x) dx $ \\ 

$ =\displaystyle\int\limits_{-\infty}^{0} ( 1- F_Y(x-z) ) 0dx + \displaystyle\int\limits_{0}^{1} ( 1- F_Y(x-z) ) 1dx + \displaystyle\int\limits_{1}^{+\infty} ( 1- F_Y(x-z) ) 0dx $ \\ 

$ = \displaystyle\int\limits_{0}^{1} ( 1- F_Y(x-z) ) dx  $  \\

Let's open $F_Y$ for better inspection:

\[  F_Y(x-z)  = \begin{cases} 
		0 & x < z-1 \\
		\dfrac{x-z+1}{2} &  z-1 \leq x < 1 + z \\
		1 & 1+z \leq x
      
   \end{cases}
\] \\

Remember that $x$ varies between $[0,1]$ and $z$ is fixed since the beginning.  So there are three cases to consider. (1) If $z>2$, $F_Y(x-z)$ evaluates to $0$ for all such $x$ and integral itself evaluates to $1$, (2) if $z<-1$, $F_Y(x-z)$ evaluates to $1$ for all such $x$ and integral itself evaluates to $0$, (3) if $z\in [-1,2]$:\\

In third case we again need to consider three cases, (1) $z-1\in [0,1]$ so $z\in [1,2]$, (2) $z+1\in [0,1]$ so $z\in [-1,0]$ and (3) $z\in [0,1]$ so $[z-1,z+1] \supset [0,1]$. Let's evaluate our integral for each case one by one. \\

(1) $F_Z(z) =  \displaystyle\int\limits_{0}^{z-1} (1-0)dx + \displaystyle\int\limits_{z-1}^{1} (1-\dfrac{x-z+1}{2}) dx $ \\ 

$ = z-1 +\dfrac{-z^2+4}{4}$ \\

(2) $F_Z(z) =  \displaystyle\int\limits_{0}^{z+1} (1-\dfrac{x-z+1}{2})  dx + \displaystyle\int\limits_{z+1}^{1} (1-1) dx $ \\ 

$ = \dfrac{z^2+2z+1}{4} +0$ \\

(3)$ F_Z(z) = \displaystyle\int\limits_{0}^{1} (1-\dfrac{x-z+1}{2}) dx $ \\ 

$ = \dfrac{z+1}{2}-\dfrac{1}{4} $ \\ 

So:

\[  F_Z(z) = \begin{cases} 
		0 & z < -1\\
        \dfrac{z^2+2z+1}{4} & -1 < z < 0 \\
       \dfrac{z+1}{2}-\dfrac{1}{4}  & 0 < z < 1 \\ 
        z-1 +\dfrac{-z^2+4}{4} & 1 < z < 2 \\
       1 & 2 < z \\
      
   \end{cases}
\] \\

\[  f_Z(z) = \begin{cases} 
		0 & z < -1\\
     	\dfrac{z+1}{2}   & -1 < z < 0 \\
       \dfrac{1}{2}& 0 < z < 1 \\ 
        \dfrac{2-z}{2} & 1 < z < 2 \\
       0 & 2 < z \\
      
   \end{cases}
\] \\

Now finally define random variable $T = |Z| = |X-Y|$ \\

$F_T(t) = P(T<t) = P(|Z|<t) = P( -t < Z < t) = F_Z(t) - F_Z(-t) $ \\

Taking derivatives of both side: \\

$f_T(t) = f_Z(t)+f_Z(-t) $\\


\[  f_T(t) = \begin{cases} 
		0 & z < -2		\\
		\dfrac{2-z}{2} & -2 < z < -1\\
     	\dfrac{z+2}{2} & -1 < z < 0 \\
       \dfrac{z+2}{2} & 0 < z < 1 \\ 
       \dfrac{2-z}{2} & 1 < z < 2 \\
       0 & 2 < z \\
      
   \end{cases}
\] \\

\section*{Answer 5}
\quad Let $E(X)=\mu_X$. Also note that expectancy satisfies the following property, where $c$ is a constant and $u$ is a function: \\

$E(c_1\cdot u_1(X) + c_2\cdot u_2(X)  ) = c_1\cdot E(u_1(X)) + c_2\cdot E( u_2(X) ) $ \\

i. By definition, we know that the right hand side equals variance: \\

$E[ (X-\mu_X)^2 ] = Var(X) = \sigma^2_X $ \\

Now let's look at $E(X^2)$:

$E(X^2) = E[ (X-\mu_X+\mu_X)^2 ] =  E[  (X-\mu_X)^2 + 2(X-\mu_X)\mu_X + \mu_X^2  ]$ \\

$= E[(X-\mu_X)^2] + 2E(X-\mu_X) + E(\mu_X^2) $\\

$ = \sigma_X^2 + 2E(X) - 2E(\mu_X) + E(\mu_X^2) $ \\

$ = \sigma_X^2 +2\mu_X -2\mu_X + \mu_X^2 = \sigma_X^2 + \mu_X^2 $ \\

Let's put all together, the left hand side is equal to:\\

$E(X^2)-E(X)^2 = \sigma_X^2 + \mu_X^2 - \mu_X^2 = \sigma_X^2 = E[ (X-\mu_X)^2 ]  $ \\

Which is just what we had said right hand side to be. \\

ii. Let $h(Y) = E_{X|Y}(X|Y) = \sum_x xP(x|Y) $, which is a random variable itself depending on the random variable $Y$. Note that the second equality is by definition of conditional expectance. Then the right hand side is:\\

$E_Y( E_{X|Y}(X|Y)  )  = E_Y(h(Y)) = \sum_y h(y)P(y) =  $\\

$ \sum_y \sum_x xP(x|y)  P(y)    = \sum_y \sum_x xP(x,y)   =\sum_x xP(x) = E(X)  $ \\

Which is just the left hand side. \\

iii. It is given that $Var(X) = Var(Y) = \sigma^2 $. Denote $Z=X-Y$ and $T=X+Y$. We wish to show $cov(Z,T)=0$.  First note that: \\

$\mu_T = E(T) = E(X+Y) = E(X) + E(Y) = \mu_X + \mu_Y $ \\

$\mu_Z = E(Z) = E(X-Y) = E(X) - E(Y) = \mu_X - \mu_Y $ \\

By definition ( Page X Formula Y book Z): \\

$cov(Z,T) = E[ (Z-\mu_Z)(T-\mu_T)] = E[ ZT - Z\mu_T - T\mu_Z +\mu_Z\mu_T    ] $ \\

$=  E(ZT) -E(Z\mu_T) - E(T\mu_Z) +E(\mu_Z\mu_T)   $ \\

$ = E[ (X-Y)(X+Y) ] - \mu_TE(X-Y) - \mu_ZE(X+Y) + \mu_Z\mu_T  $ \\

$ = E(X^2) - E(Y^2  ) - \mu_TE(X-Y) - \mu_ZE(X+Y) + \mu_Z\mu_T   $ \\ 

$ = E(X^2) - E(Y^2  ) - \mu_TE(X) + \mu_TE(Y)- \mu_ZE(X) - \mu_ZE(Y) + \mu_Z\mu_T   $ \\ 

$ = E(X^2) - E(Y^2  ) - (\mu_X + \mu_Y)E(X) + (\mu_X + \mu_Y) E(Y)- (\mu_X - \mu_Y) E(X) - (\mu_X - \mu_Y) E(Y) + (\mu_X - \mu_Y)(\mu_X + \mu_Y)   $ \\ 

$ = E(X^2) - E(Y^2  ) - (\mu_X + \mu_Y) \mu_X+ (\mu_X + \mu_Y) \mu_Y- (\mu_X - \mu_Y) \mu_X - (\mu_X - \mu_Y) \mu_Y + (\mu_X - \mu_Y)(\mu_X + \mu_Y)   $ \\ 

$ = E(X^2) - E(Y^2  ) - \mu_X^2 - \mu_Y\mu_X+ \mu_X\mu_Y + \mu_Y^2  -\mu_X^2 +\mu_Y\mu_X    -\mu_X\mu_Y  + \mu_Y^2 +        \mu_X^2 - \mu_Y^2 $ \\ 

$ = E(X^2) - E(Y^2  ) - \mu_X^2 + \mu_Y^2  $ \\ 

Remember in part $i$ we showed that $E(X^2) = \mu_X^2 + \sigma_X^2$  \\

$ = \mu_X^2 + \sigma^2 - \mu_Y^2 - \sigma^2 - \mu_X^2 + \mu_Y^2 = 0 $ \\

Which means $T$ and $Z$ has zero co-variance and therefore are uncorrelated.\\

\section*{Answer 6}

\quad i. Independence is the case iff $P(X=x,Y=y)=P(X=x)P(Y=y) $ \\

Uncorrelated is the case iff $cov(X,Y) = E(XY) - E(X)E(Y) = 0$ or $E(XY) = E(X)E(Y) $. \\

Let's start with left hand side: \\

$E(XY) = \displaystyle \int \int xyf_{X,Y}(x,y)dxdy $ \\

If $X$ and $Y$ are independent,we can proceed as follows:

$ = \displaystyle \int \int xyf_X(x)f_Y(y)dxdy  $ \\

$ = \displaystyle \int x f_X(x) (\int y f_Y(y) dy) dx  $  \\

$ = \displaystyle  (\int y f_Y(y) dy)  (\int x f_X(x) dx)  $ \\

$ = E(X)E(Y) $ \\

We reached exactly to right hand side.   \\

ii. Uncorrelation is simply a formal measure, in a sense less correlated they are, more they have a "linear relationship". Dependence on the other hand directly measures whether one affects the other. \\

Take $X \sim U(-1,1)$ and $X^2$ for example. They can be shown to be uncorrelated: \\

$cov(X,X^2) = E(X^3) - E(X)E(X^2)= 0 - 0 = 0 $ \\

Because $E(X)=0$ and it is easy to see that $X^3 \sim U(-1,1)$ \\

Yet the random variables are clearly dependent.\\

iii. Let us toss 2 fair coins. Let $A$ be the random variable indicating the result of the first coin, and $B$ of second coin. Then let $C$ be the random variable showing whether the two coins lands on the same state, head or tail. Let $A=1$ implies head, and  $0$ tail. \\

$P(A=1) = P(A=0) = P(B=1) = P(B=0) = P(C=1) = P(C=0) = \dfrac{1}{2} $ \\

\begin{table}[]
\begin{tabular}{|l|l|l|l|}
\hline
A & B & C & Prob. \\ \hline
0 & 0 & 0 &      Can't Happen   \\ \hline
0 & 0 & 1 &        $\dfrac{1}{4}$ \\ \hline
0 & 1 & 0 &       $\dfrac{1}{4}$  \\ \hline
0 & 1 & 1 &       Can't Happen \\ \hline
1 & 0 & 0 &       $\dfrac{1}{4}$  \\ \hline
1 & 0 & 1 &       Can't Happen  \\ \hline
1 & 1 & 0 &        Can't Happen \\ \hline
1 & 1 & 1 &        $\dfrac{1}{4}$ \\ \hline
\end{tabular}
\caption{3-way joint probabilities }
\label{my-label}
\end{table}

All events are clearly piece-wise independent, let's show some examples(joint probabilities can be calculated by using the table):

$P(A=1, B=1) = \dfrac{1}{4} = P(A=1)P(B=1) = \dfrac{1}{2}\cdot \dfrac{1}{2} $ \\

$P(A=1,C=0) =  \dfrac{1}{4} = P(A=1)P(C=0) = \dfrac{1}{2} \cdot \dfrac{1}{2} $ \\

Yet are they mutually independent?\\

$P(A=1,B=1,C=1) = \dfrac{1}{4} \neq P(A=1)P(B=1)P(C=1) = \dfrac{1}{8} $  \\

Turns out not!


\section*{Answer 7}

The question can be formulated as; given random variables $X \sim U(0,1)$ and $Y \sim U(0,1)$, what is $E(|X-Y|)$? \\

Let us define: \\

\[  h(x,y) = \begin{cases} 
		x-y & y \leq x \\
		y-x & x < y       
   \end{cases}
\] \\

Which is just the absolute operator. Then:

$E(|X-Y|) = E(h(X,Y)) = \displaystyle \int_0^1 \int_0^1 h(x,y) f_{X,Y}(x,y) dx dy $ \\

Then since $X$ and $Y$ are independent;\\

$ = \displaystyle \int_0^1 \int_0^1 h(x,y) f_X(x) f_Y(y) dx dy $ \\

And we know the pdf of $X$ and $Y$ since they are known uniform distributions: \\

$ = \displaystyle \int_0^1 \int_0^1 h(x,y) \dfrac{1}{1} \dfrac{1}{1} dx dy $ \\

$ = \displaystyle \int_0^1 \int_0^1 h(x,y) dx dy $ \\

$ =  \displaystyle \int_0^1 \int_0^y (y-x) dx dy  +  \displaystyle \int_0^1 \int_y^1 (x-y) dx dy $ \\

$ =    \displaystyle \int_0^1 \dfrac{y^2}{2} dy  +  \displaystyle \int_0^1 -y+\frac{1+y^2}{2} dy $ \\

$ = \dfrac{1}{6} + \dfrac{1}{6} = \dfrac{1}{3} $

\section*{Answer 8}

The number of times we do the selection until we hit the ball $20$ is a random variable $X$. Since we stop at first success, and each selection has the same probability of success, $X$ has a Geometric distribution with parameter $p=\dfrac{1}{20}$. \\

i. By the properties of geometric distribution, $P(X=n) = (1-p)^{n-1}p = (\dfrac{19}{20})^{n-1}\cdot \dfrac{1}{20}$ \\

Intuitively, we need  $n-1$ times failure and $1$ times success, which means $1-p$ multiplied $n-1$ times, times $p$. \\

ii. $E(X) = \dfrac{1}{\dfrac{1}{20}} = 20 $ \\

As given by the expectancy formula for geometric distribution.\\



\section*{Answer 9}

Changing means simple changes where the tip of the gaussian resides. \\

i .Changing either variance scales the respective axis, figure 1 shows the case with 0 covariance and first variance is bigger than second. Even tough the gaussian may look normal(pun intended), notice that the X and Y scales are different. Tuning the axis to same scale would result in, in this case, the gaussian being stretched along the X dimension (top-right to bottom-left dimension)\\

ii. Further, introducing covariance stretches the gaussian diagonally, in a sense along both axis simultaneously, which means "bounding the high probabilities" together. Increasing covariance all the way to 1 produces a straight line where all the probabilities clumps up.

Further more, we can notice that, with more experiments; increasing both the covariance and the variance of a single axis produces a "linear effect", in another words they don't "amplify each other". Linear in the sense that increasing covariance do not further scale the axises(ie. variance), and scaling the axises(ie. variance) do not further "clump the gaussin"(ie. covariance).

\begin{figure}
  \includegraphics[width=\linewidth]{1.png}
  \caption{ 0 covariance, $\sigma_X > \sigma_Y$}
  \label{fig:boat1}
\end{figure}


\begin{figure}
  \includegraphics[width=\linewidth]{2.png}
  \caption{ 0.5 covariance, $\sigma_X = \sigma_Y$}
  \label{fig:boat2}
\end{figure}

\section*{Answer 10}

\section*{Answer 11}

\section*{Answer 12}

\section*{Answer 13}

\section*{Answer 14}

\section*{Answer 15}

\section*{Answer 16 - BONUS}



\end{document}


